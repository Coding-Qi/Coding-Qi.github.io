<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Pytorch官方文档学习笔记_3_Build Model</title>
    <link href="/2022/07/21/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-Build-Model/"/>
    <url>/2022/07/21/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-Build-Model/</url>
    
    <content type="html"><![CDATA[<h1 id="1-建立神经网络"><a href="#1-建立神经网络" class="headerlink" title="1. 建立神经网络"></a>1. 建立神经网络</h1><p><a href="https://pytorch.org/docs/stable/nn.html">torch.nn</a> 提供了构建神经网络所需的所有构建块。PyTorch中的每个模块都是 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">nn.Module</a> 的子类。</p><p>在接下来的部分中，我们将构建一个神经网络来对 FashionMNIST 数据集中的图像进行分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets, transforms<br></code></pre></td></tr></table></figure><h1 id="2-获取训练设备"><a href="#2-获取训练设备" class="headerlink" title="2. 获取训练设备"></a>2. 获取训练设备</h1><p>我们通常希望能够在GPU等硬件加速器（如果可用）上训练模型。首先检查一下 torch.cuda 是否可用，否则就继续使用CPU。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br></code></pre></td></tr></table></figure><h1 id="3-定义类"><a href="#3-定义类" class="headerlink" title="3. 定义类"></a>3. 定义类</h1><p>我们通过子类化 <strong>nn.Module</strong> 来定义我们的神经网络，并在 <strong><strong>init</strong></strong> 中初始化神经网络层。每个 <strong>nn.Module</strong> 子类都在 <strong>forward</strong> 方法中实现对输入数据的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">NeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(NeuralNetwork, self).__init__()<br>        self.flatten = nn.Flatten()<br>        self.linear_relu_stack = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">28</span>*<span class="hljs-number">28</span>, <span class="hljs-number">512</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">10</span>),<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.flatten(x)<br>        logits = self.linear_relu_stack(x)<br>        <span class="hljs-keyword">return</span> logits<br></code></pre></td></tr></table></figure><p>我们创建一个 <strong>NeuralNetwork</strong> 实例，并将其移动到 <strong>device</strong> 上，并打印其结构。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">model = NeuralNetwork().to(device)<br><span class="hljs-built_in">print</span>(model)<br><br>out: <br>&gt;&gt; NeuralNetwork(<br>     (flatten): Flatten(start_dim=<span class="hljs-number">1</span>, end_dim=-<span class="hljs-number">1</span>)<br>     (linear_relu_stack): Sequential(<br>       (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">784</span>, out_features=<span class="hljs-number">512</span>, bias=<span class="hljs-literal">True</span>)<br>       (<span class="hljs-number">1</span>): ReLU()<br>       (<span class="hljs-number">2</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>, bias=<span class="hljs-literal">True</span>)<br>       (<span class="hljs-number">3</span>): ReLU()<br>       (<span class="hljs-number">4</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">10</span>, bias=<span class="hljs-literal">True</span>)<br>     )<br>   )<br></code></pre></td></tr></table></figure><p>要使用模型，我们将输入数据传递给它。这将执行模型的转发，以及一些后台操作。</p><p>注意：请不要直接调用 <strong>model.forward()</strong> ！</p><p>在输入上调用模型会返回一个 10 维张量，其中包含每个类的原始预测值。我们通过一个 <strong>nn.Softmax</strong> 模块的实例来获得预测概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, device=device)<br>logits = model(X)<br>pred_probab = nn.Softmax(dim=<span class="hljs-number">1</span>)(logits)<br>y_pred = pred_probab.argmax(<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Predicted class: <span class="hljs-subst">&#123;y_pred&#125;</span>&quot;</span>)<br><br>out: <br>&gt;&gt; Predicted <span class="hljs-keyword">class</span>: tensor([<span class="hljs-number">9</span>], device=<span class="hljs-string">&#x27;cuda:0&#x27;</span>)<br></code></pre></td></tr></table></figure><h1 id="4-模型层"><a href="#4-模型层" class="headerlink" title="4. 模型层"></a>4. 模型层</h1><p>让我们分解 FashionMNIST 模型中的层。为了方便说明，我们将抽取 3 张大小为 28x28 的图像的小批量样本，看看当我们通过网络传递它时会发生什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">input_image = torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)<br></code></pre></td></tr></table></figure><h2 id="4-1-nn-Flatten"><a href="#4-1-nn-Flatten" class="headerlink" title="4.1 nn.Flatten"></a>4.1 nn.Flatten</h2><p>我们初始化 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html">nn.Flatten</a> 层以将每个 2D 28x28 图像转换为 784 个像素值的连续数组（保持小批量维度（$dim&#x3D;0$））。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">flatten = nn.Flatten()<br>flat_image = flatten(input_image)<br><span class="hljs-built_in">print</span>(flat_image.size())<br><br>out: <br>&gt;&gt; torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">784</span>])<br></code></pre></td></tr></table></figure><h2 id="4-2-nn-Linear"><a href="#4-2-nn-Linear" class="headerlink" title="4.2 nn.Linear"></a>4.2 nn.Linear</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">linear</a> 层是一个使用其存储的权重和偏置对输入进行线性转换的模块，即 $y&#x3D;wx+b$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">layer1 = nn.Linear(in_features=<span class="hljs-number">784</span>, out_features=<span class="hljs-number">20</span>)<br>hidden1 = layer1(flat_image)<br><span class="hljs-built_in">print</span>(hidden1.size())<br><br>&gt;&gt; torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">20</span>])<br></code></pre></td></tr></table></figure><h2 id="4-3-nn-ReLU"><a href="#4-3-nn-ReLU" class="headerlink" title="4.3 nn.ReLU"></a>4.3 nn.ReLU</h2><p>依靠非线性激活函数可以在模型的输入和输出之间创建复杂映射，在线性变换后应用以引入非线性，可以帮助神经网络学习各种复杂的现象，更多常见的激活函数及其介绍可见<a href="https://blog.csdn.net/m0_60882889/article/details/120502609?spm=1001.2014.3001.5501">这里</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">hidden1 = nn.ReLU()(hidden1)<br></code></pre></td></tr></table></figure><h2 id="4-4-nn-Sequential"><a href="#4-4-nn-Sequential" class="headerlink" title="4.4 nn.Sequential"></a>4.4 nn.Sequential</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html">nn.Sequential</a> 是一个有序的模块容器。数据按照定义的顺序通过所有模块，您可以使用顺序容器来组合一个如同 <strong>seq_modules</strong> 的快速网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">seq_modules = nn.Sequential(<br>    flatten,<br>    layer1,<br>    nn.ReLU(),<br>    nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">10</span>)<br>)<br>input_image = torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)<br>logits = seq_modules(input_image)<br></code></pre></td></tr></table></figure><h2 id="4-5-nn-Softmax"><a href="#4-5-nn-Softmax" class="headerlink" title="4.5 nn.Softmax"></a>4.5 nn.Softmax</h2><p>上述的神经网络的最后一层的线性层返回了取值在 [-infty, infty] 的未处理的数值 —— <em>logits</em>。将 <em>logits</em> 传给 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html">nn.Softmax</a> 模块后取值被缩放到 [0, 1] 内，以表示模型对每个类别的预测概率。<strong>dim</strong> 参数指示维度的数值总和必须为 1 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">softmax = nn.Softmax(dim=<span class="hljs-number">1</span>)<br>pred_probab = softmax(logits)<br></code></pre></td></tr></table></figure><h1 id="5-模型参数"><a href="#5-模型参数" class="headerlink" title="5. 模型参数"></a>5. 模型参数</h1><p>神经网络中的许多层都是参数化的，即具有在训练期间优化的相关权重和偏差。子类化 nn.Module 会自动跟踪模型对象中定义的所有字段，并使用模型的 parameters() 或 named_parameters() 方法使所有参数都可以访问。</p><p>在此示例中，我们遍历每个参数，并打印其大小和其值的预览。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Model structure: <span class="hljs-subst">&#123;model&#125;</span>\n\n&quot;</span>)<br><br><span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Layer: <span class="hljs-subst">&#123;name&#125;</span> | Size: <span class="hljs-subst">&#123;param.size()&#125;</span> | Values : <span class="hljs-subst">&#123;param[:<span class="hljs-number">2</span>]&#125;</span> \n&quot;</span>)<br></code></pre></td></tr></table></figure><p>Out： </p><blockquote><p>Model structure: NeuralNetwork(<br>  (flatten): Flatten(start_dim&#x3D;1, end_dim&#x3D;-1)<br>  (linear_relu_stack): Sequential(<br>   (0): Linear(in_features&#x3D;784, out_features&#x3D;512, bias&#x3D;True)<br>   (1): ReLU()<br>   (2): Linear(in_features&#x3D;512, out_features&#x3D;512, bias&#x3D;True)<br>   (3): ReLU()<br>   (4): Linear(in_features&#x3D;512, out_features&#x3D;10, bias&#x3D;True)<br>  )<br> )</p><p> Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0033, -0.0081, -0.0354,  …, -0.0335,  0.0070,  0.0030],<br>     [ 0.0106, -0.0064,  0.0300,  …,  0.0071, -0.0062,  0.0169]],<br>     device&#x3D;’cuda:0’, grad_fn&#x3D;<SliceBackward0>)</p><p>Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0193, -0.0153], device&#x3D;’cuda:0’, grad_fn&#x3D;<SliceBackward0>)</p><p>Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0408,  0.0078,  0.0300,  …,  0.0058, -0.0142, -0.0226],<br>     [ 0.0319, -0.0063, -0.0093,  …, -0.0096,  0.0352,  0.0178]],<br>     device&#x3D;’cuda:0’, grad_fn&#x3D;<SliceBackward0>)</p><p>Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0219, 0.0020], device&#x3D;’cuda:0’, grad_fn&#x3D;<SliceBackward0>)</p><p>Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0076,  0.0076,  0.0433,  …,  0.0178,  0.0230,  0.0227],<br>     [-0.0396, -0.0042,  0.0342,  …, -0.0364, -0.0184, -0.0329]],<br>     device&#x3D;’cuda:0’, grad_fn&#x3D;<SliceBackward0>)</p><p>Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0380, -0.0044], device&#x3D;’cuda:0’, grad_fn&#x3D;<SliceBackward0>)</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch官方文档学习笔记_2_Datasets&amp;DataLoaders&amp;Transforms</title>
    <link href="/2022/07/21/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-Datasets-DataLoaders-Transforms/"/>
    <url>/2022/07/21/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-Datasets-DataLoaders-Transforms/</url>
    
    <content type="html"><![CDATA[<p>PyTorch提供了两个数据原语允许您使用预加载的数据和您自己的数据：<strong>torch.utils.data.DataLoader</strong> 和 <strong>torch.utils.data.Dataset</strong> 。</p><p>PyTorch域库提供了许多预加载的数据集（例如 FashionMNIST），这些数据集是 <strong>torch.utils.data.Dataset</strong> 的子类，并对于特定数据实现了特定的功能。它们可用于对您的模型进行原型设计和基准测试。可以在此处找到它们：<a href="https://pytorch.org/vision/stable/datasets.html">图像数据集</a>、<a href="https://pytorch.org/text/stable/datasets.html">文本数据集</a>和<a href="https://pytorch.org/audio/stable/datasets.html">音频数据集</a>。</p><h1 id="1-加载数据集"><a href="#1-加载数据集" class="headerlink" title="1. 加载数据集"></a>1. 加载数据集</h1><p>下面以如何从 TorchVision 加载 Fashion-MNIST 数据集为例。Fashion-MNIST 由 60,000 个训练示例和 10,000 个测试示例组成。每个示例都包含 28×28 灰度图像和来自 10 个类别之一的相关标签。</p><p>我们使用以下参数加载 FashionMNIST 数据集：</p><ul><li><strong>root</strong> 是存储训练&#x2F;测试数据的路径</li><li><strong>train</strong> 指定训练或测试数据集（True表示训练数据集，False表示测试数据集）</li><li><strong>download&#x3D;True</strong> 指如果根目录下不可用，则从 Internet 下载数据</li><li><strong>transform</strong> 和 <strong>target_transform</strong> 指定了特征和标签的转换。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><br>training_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">True</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br><br>test_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">False</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br></code></pre></td></tr></table></figure><h1 id="2-迭代和可视化数据集"><a href="#2-迭代和可视化数据集" class="headerlink" title="2. 迭代和可视化数据集"></a>2. 迭代和可视化数据集</h1><p>可以如同 list 一样手动索引 <strong>Datasets : training_data[index]。</strong>我们使用 <strong>matplotlib</strong> 来可视化我们训练数据中的一些样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">labels_map = &#123;<br>    <span class="hljs-number">0</span>: <span class="hljs-string">&quot;T-Shirt&quot;</span>,<br>    <span class="hljs-number">1</span>: <span class="hljs-string">&quot;Trouser&quot;</span>,<br>    <span class="hljs-number">2</span>: <span class="hljs-string">&quot;Pullover&quot;</span>,<br>    <span class="hljs-number">3</span>: <span class="hljs-string">&quot;Dress&quot;</span>,<br>    <span class="hljs-number">4</span>: <span class="hljs-string">&quot;Coat&quot;</span>,<br>    <span class="hljs-number">5</span>: <span class="hljs-string">&quot;Sandal&quot;</span>,<br>    <span class="hljs-number">6</span>: <span class="hljs-string">&quot;Shirt&quot;</span>,<br>    <span class="hljs-number">7</span>: <span class="hljs-string">&quot;Sneaker&quot;</span>,<br>    <span class="hljs-number">8</span>: <span class="hljs-string">&quot;Bag&quot;</span>,<br>    <span class="hljs-number">9</span>: <span class="hljs-string">&quot;Ankle Boot&quot;</span>,<br>&#125;<br>figure = plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br>cols, rows = <span class="hljs-number">3</span>, <span class="hljs-number">3</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, cols * rows + <span class="hljs-number">1</span>):<br>    sample_idx = torch.randint(<span class="hljs-built_in">len</span>(training_data), size=(<span class="hljs-number">1</span>,)).item()<br>    img, label = training_data[sample_idx]<br>    figure.add_subplot(rows, cols, i)<br>    plt.title(labels_map[label])<br>    plt.axis(<span class="hljs-string">&quot;off&quot;</span>)<br>    plt.imshow(img.squeeze(), cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><center><img src="/post_img/Pytorch官方文档学习笔记_2_Datasets&DataLoaders&Transforms/picture1.png" alt="img" style="zoom:50%;" /></center><h1 id="3-创建自定义数据集"><a href="#3-创建自定义数据集" class="headerlink" title="3. 创建自定义数据集"></a>3. 创建自定义数据集</h1><p>自定义数据集类必须实现三个函数：*<strong>init</strong><em>、</em><strong>len</strong>* 和 *<strong>getitem</strong>*。看一下如下这个实现，FashionMNIST 图像存储在目录 <strong>img_dir</strong> 中，它们的标签存储在 CSV 文件 <strong>annotations_file</strong> 中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> torchvision.io <span class="hljs-keyword">import</span> read_image<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomImageDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, annotations_file, img_dir, </span><br><span class="hljs-params">                    transform=<span class="hljs-literal">None</span>, target_transform=<span class="hljs-literal">None</span></span>):<br>        self.img_labels = pd.read_csv(annotations_file)<br>        self.img_dir = img_dir<br>        self.transform = transform<br>        self.target_transform = target_transform<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.img_labels)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="hljs-number">0</span>])<br>        image = read_image(img_path)<br>        label = self.img_labels.iloc[idx, <span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> self.transform:<br>            image = self.transform(image)<br>        <span class="hljs-keyword">if</span> self.target_transform:<br>            label = self.target_transform(label)<br>        <span class="hljs-keyword">return</span> image, label<br></code></pre></td></tr></table></figure><p>接下来我们将分解介绍每个部分发生了什么。</p><h2 id="3-1-init"><a href="#3-1-init" class="headerlink" title="3.1 init"></a>3.1 <strong>init</strong></h2><p><strong>init</strong> 函数在实例化 Dataset 对象时运行一次。我们初始化了包含图像、注释文件和两种转换的目录。</p><h2 id="3-2-len"><a href="#3-2-len" class="headerlink" title="3.2 len"></a>3.2 <strong>len</strong></h2><p><strong>len</strong> 函数返回了我们数据集中的样本数。</p><h2 id="3-3-getitem"><a href="#3-3-getitem" class="headerlink" title="3.3 getitem"></a>3.3 <strong>getitem</strong></h2><p><strong>getitem</strong> 函数从给定索引 idx 的数据集中加载并返回一个样本。根据索引，它识别图像在磁盘上的位置，使用 <strong>read_image</strong> 将其转换为张量，从 <strong>self.img_labels</strong> 中的 csv 数据中检索相应的标签，调用它们的转换函数（如果适用），并返回张量图像和元组中的相应标签。</p><h1 id="4-使用-DataLoaders-为训练准备数据"><a href="#4-使用-DataLoaders-为训练准备数据" class="headerlink" title="4. 使用 DataLoaders 为训练准备数据"></a>4. 使用 DataLoaders 为训练准备数据</h1><p><strong>Dataset</strong> 每次检索一个样本的数据集特征和标签。在训练模型时，我们通常希望以“小批量的形式传递样本，在每个 epoch 重新洗牌以减少模型过度拟合，并使用 Python 的 <strong>multiprocessing</strong> 来加速数据检索。</p><p><strong>DataLoader</strong> 是一个迭代器，它通过一个简单的 API 为我们抽象了这种复杂性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br>train_dataloader = DataLoader(training_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br>test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h1 id="5-遍历-DataLoader"><a href="#5-遍历-DataLoader" class="headerlink" title="5. 遍历 DataLoader"></a>5. 遍历 DataLoader</h1><p>下面的每次迭代都会返回一批 train_features 和 train_labels（分别包含 batch_size&#x3D;64 个特征和标签）。因为我们指定了 shuffle&#x3D;True，所以在遍历所有批次之后，数据会被打乱（为了更细粒度地控制数据加载顺序，请查看 <a href="https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler">Samplers</a>）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Display image and label.</span><br>train_features, train_labels = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(train_dataloader))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Feature batch shape: <span class="hljs-subst">&#123;train_features.size()&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Labels batch shape: <span class="hljs-subst">&#123;train_labels.size()&#125;</span>&quot;</span>)<br>img = train_features[<span class="hljs-number">0</span>].squeeze()<br>label = train_labels[<span class="hljs-number">0</span>]<br>plt.imshow(img, cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br>plt.show()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Label: <span class="hljs-subst">&#123;label&#125;</span>&quot;</span>)<br><br><br>out: <br>&gt;&gt; Feature batch shape: torch.Size([<span class="hljs-number">64</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br>&gt;&gt; Labels batch shape: torch.Size([<span class="hljs-number">64</span>])<br>&gt;&gt; Label: <span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><center><img src="/post_img/Pytorch官方文档学习笔记_2_Datasets&DataLoaders&Transforms/picture2.png" style="zoom:50%;" /></center><h1 id="6-Transforms"><a href="#6-Transforms" class="headerlink" title="6. Transforms"></a>6. Transforms</h1><p>数据通常不是以训练机器学习算法所需的最终处理的形式出现。我们使用 <strong>transforms</strong> 来对数据进行一些操作并使其适合训练。</p><p>所有 TorchVision 数据集都有两个参数 —— 用于修改特征的 <strong>transform</strong> 和用于修改标签的 <strong>target_transform</strong> —— 接受包含转换逻辑的可调用对象。<a href="https://pytorch.org/vision/stable/transforms.html">torchvision.transforms</a>模块提供了几个开箱即用的常用转换。</p><p>FashionMNIST 特征是 PIL 图像格式，标签是整数。为了训练我们需要将特征作为归一化张量，并将标签作为 one-hot 编码张量。为了进行这些转换，我们使用 <strong>ToTensor</strong> 和 <strong>Lambda</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor, Lambda<br><br>ds = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">True</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor(),<br>    target_transform=Lambda(<span class="hljs-keyword">lambda</span> y: <br>            torch.zeros(<span class="hljs-number">10</span>,dtype=torch.<span class="hljs-built_in">float</span>).scatter_(<span class="hljs-number">0</span>,torch.tensor(y),value=<span class="hljs-number">1</span>)<br>    )<br>)<br></code></pre></td></tr></table></figure><h2 id="6-1-ToTensor"><a href="#6-1-ToTensor" class="headerlink" title="6.1 ToTensor()"></a>6.1 ToTensor()</h2><p>ToTensor 将 PIL 图像或 NumPy ndarray 转换为 FloatTensor，并在 [0., 1.] 范围内缩放图像的像素强度值。</p><h2 id="6-2-Lambda-Transforms"><a href="#6-2-Lambda-Transforms" class="headerlink" title="6.2 Lambda Transforms"></a>6.2 Lambda Transforms</h2><p>Lambda 转换可使用任何用户定义的 lambda 函数。这里，我们定义了一个函数来将整数转换为 one-hot 编码张量。它首先创建一个大小为 10（我们数据集中的标签数量）的零张量，并调用 **scatter_**，它在标签 y 给定的索引上分配 value&#x3D;1。</p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch官方文档学习笔记_1_Tensor</title>
    <link href="/2022/07/21/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-Tensor/"/>
    <url>/2022/07/21/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-Tensor/</url>
    
    <content type="html"><![CDATA[<h1 id="1-初始化Tnesor"><a href="#1-初始化Tnesor" class="headerlink" title="1. 初始化Tnesor"></a>1. 初始化Tnesor</h1><h2 id="1-1-直接从数据"><a href="#1-1-直接从数据" class="headerlink" title="1.1 直接从数据"></a>1.1 直接从数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">data = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br>x_data = torch.tensor(data)<br></code></pre></td></tr></table></figure><h2 id="1-2-来自Numpy数组"><a href="#1-2-来自Numpy数组" class="headerlink" title="1.2 来自Numpy数组"></a>1.2 来自Numpy数组</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">np_array = np.array(data)<br>x_np = torch.from_numpy(np_array)<br></code></pre></td></tr></table></figure><h2 id="1-3-来自另一个Tensor"><a href="#1-3-来自另一个Tensor" class="headerlink" title="1.3 来自另一个Tensor"></a>1.3 来自另一个Tensor</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># retains the properties of x_data</span><br>x_ones = torch.ones_like(x_data) <br><br><span class="hljs-comment"># overrides the datatype of x_data</span><br>x_rand = torch.rand_like(x_data, dtype=torch.<span class="hljs-built_in">float</span>) <br></code></pre></td></tr></table></figure><h2 id="1-4-使用随机值或固定值"><a href="#1-4-使用随机值或固定值" class="headerlink" title="1.4 使用随机值或固定值"></a>1.4 使用随机值或固定值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">shape = (<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,)<br>rand_tensor = torch.rand(shape)<br>ones_tensor = torch.ones(shape)<br>zeros_tensor = torch.zeros(shape)<br></code></pre></td></tr></table></figure><h1 id="2-Tensor的属性"><a href="#2-Tensor的属性" class="headerlink" title="2. Tensor的属性"></a>2. Tensor的属性</h1><p>Tensor属性描述了它们的形状、数据类型和存储它们的设备。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor = torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><br><span class="hljs-built_in">print</span>(tensor.shape)<br><span class="hljs-built_in">print</span>(tensor.dtype)<br><span class="hljs-built_in">print</span>(tensor.device)<br><br>out：<br>&gt;&gt; torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br>&gt;&gt; torch.float32<br>&gt;&gt; cpu<br></code></pre></td></tr></table></figure><h1 id="3-Tensor的运算"><a href="#3-Tensor的运算" class="headerlink" title="3. Tensor的运算"></a>3. Tensor的运算</h1><p>更全面的Tensor运算介绍可点击<a href="https://pytorch.org/docs/stable/torch.html">这里</a>。</p><p>所有Tensor运算均可以在GPU上运行，并且通常有比CPU更高的速度。默认情况下，Tensor是在 CPU 上创建的。我们需要使用 <strong>.to</strong> 方法（在检查 GPU 可用性之后）将Tensor显式移动到 GPU上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    tensor = tensor.to(<span class="hljs-string">&quot;cuda&quot;</span>)<br></code></pre></td></tr></table></figure><h2 id="3-1-类似-numpy-的索引和切片"><a href="#3-1-类似-numpy-的索引和切片" class="headerlink" title="3.1 类似 numpy 的索引和切片"></a>3.1 类似 numpy 的索引和切片</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor = torch.ones(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;First row: <span class="hljs-subst">&#123;tensor[<span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;First column: <span class="hljs-subst">&#123;tensor[:, <span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Last column: <span class="hljs-subst">&#123;tensor[..., -<span class="hljs-number">1</span>]&#125;</span>&quot;</span>)<br>tensor[:,<span class="hljs-number">1</span>] = <span class="hljs-number">0</span><br><span class="hljs-built_in">print</span>(tensor)<br><br>out：<br>&gt;&gt; First row: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>&gt;&gt; First column: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>&gt;&gt; Last column: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>&gt;&gt; tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>           [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>           [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>           [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br></code></pre></td></tr></table></figure><h2 id="3-2-链接Tensor"><a href="#3-2-链接Tensor" class="headerlink" title="3.2 链接Tensor"></a>3.2 链接Tensor</h2><p>可以使用 <strong>torch.cat</strong> 沿给定维度连接一系列Tensor。此外，还有一个与 <strong>torch.cat</strong> 略有不同的，可以在新的维度链接Tensor的操作可参阅<a href="https://blog.csdn.net/m0_60882889/article/details/120899946?spm=1001.2014.3001.5501">这里</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(t1)<br><br>out：<br>&gt;&gt; tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>           [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>           [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>           [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br></code></pre></td></tr></table></figure><h2 id="3-3-算术运算"><a href="#3-3-算术运算" class="headerlink" title="3.3 算术运算"></a>3.3 算术运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 计算两个矩阵相乘。 y1, y2, y3具有相同的值。</span><br>y1 = tensor @ tensor.T <span class="hljs-comment"># tensor.T是计算Tensor的转置</span><br>y2 = tensor.matmul(tensor.T)<br><br>y3 = torch.rand_like(tensor) <span class="hljs-comment"># tensor.shape是两矩阵相乘后的shape</span><br>torch.matmul(tensor, tensor2.T, out=y)<br><br><br><span class="hljs-comment"># 这计算的是矩阵元素逐一相乘。 z1, z2, z3具有相同的值。</span><br>z1 = tensor * tensor<br>z2 = tensor.mul(tensor)<br><br>z3 = torch.rand_like(tensor)<br>torch.mul(tensor, tensor, out=z3)<br></code></pre></td></tr></table></figure><h2 id="3-4-转化为Python-数值"><a href="#3-4-转化为Python-数值" class="headerlink" title="3.4 转化为Python 数值"></a>3.4 转化为Python 数值</h2><p>通过 item() 可以讲单元素Tensor转化为Python数值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">agg = tensor.<span class="hljs-built_in">sum</span>() <span class="hljs-comment"># 通过将tensor的所有值聚合成一个值</span><br>agg_item = agg.item()<br><span class="hljs-built_in">print</span>(agg_item, <span class="hljs-built_in">type</span>(agg_item))<br><br>out: <br>&gt;&gt; <span class="hljs-number">12.0</span> &lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;float&#x27;</span>&gt;<br></code></pre></td></tr></table></figure><h2 id="3-5-就地操作"><a href="#3-5-就地操作" class="headerlink" title="3.5 就地操作"></a>3.5 就地操作</h2><p>将操作结果储存到操作数的操作成为就地操作。常用_后缀表示，例如：x.copy_(y)，x.t_()，会直接改变x。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(tensor, <span class="hljs-string">&quot;\n&quot;</span>)<br>tensor.add_(<span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(tensor)<br><br>out: <br>&gt;&gt;tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>          [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>          [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>          [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br><br><br>&gt;&gt;tensor([[<span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>],<br>          [<span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>],<br>          [<span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>],<br>          [<span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>]])<br></code></pre></td></tr></table></figure><p>Note: 就地操作可以节约一些内存，但是计算导数时可能出现问题，因为会立即丢失历史记录。因此不鼓励使用它们。</p><h1 id="4-与Numpy桥接"><a href="#4-与Numpy桥接" class="headerlink" title="4. 与Numpy桥接"></a>4. 与Numpy桥接</h1><p>CPU上的Tensor和Numpy数组可以共享他们的底层内存位置，并且改变其中一个另一个也会改变。</p><h2 id="4-1-Tensor到Numpy数组"><a href="#4-1-Tensor到Numpy数组" class="headerlink" title="4.1 Tensor到Numpy数组"></a>4.1 Tensor到Numpy数组</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">t = torch.ones(<span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;t: <span class="hljs-subst">&#123;t&#125;</span>&quot;</span>)<br>n = t.numpy()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;n: <span class="hljs-subst">&#123;n&#125;</span>&quot;</span>)<br><br>out: <br>&gt;&gt; t: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>&gt;&gt; n: [<span class="hljs-number">1.</span> <span class="hljs-number">1.</span> <span class="hljs-number">1.</span> <span class="hljs-number">1.</span> <span class="hljs-number">1.</span>]<br></code></pre></td></tr></table></figure><p>Tensor的变化会反映在Numpy数组中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">t.add_(<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;t: <span class="hljs-subst">&#123;t&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;n: <span class="hljs-subst">&#123;n&#125;</span>&quot;</span>)<br><br>out: <br>&gt;&gt; t: tensor([<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>])<br>&gt;&gt; n: [<span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span>]<br></code></pre></td></tr></table></figure><h2 id="4-2-Numpy数组到Tensor"><a href="#4-2-Numpy数组到Tensor" class="headerlink" title="4.2 Numpy数组到Tensor"></a>4.2 Numpy数组到Tensor</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">n = np.ones(<span class="hljs-number">5</span>)<br>t = torch.from_numpy(n)<br></code></pre></td></tr></table></figure><p>NumPy 数组的变化反映在Tensor中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">np.add(n, <span class="hljs-number">1</span>, out=n)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;t: <span class="hljs-subst">&#123;t&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;n: <span class="hljs-subst">&#123;n&#125;</span>&quot;</span>)<br><br>out: <br>&gt;&gt; t: tensor([<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>], dtype=torch.float64)<br>&gt;&gt; n: [<span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span>]<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
