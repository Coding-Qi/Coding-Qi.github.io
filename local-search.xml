<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Pytorch官方文档学习笔记_6_Save&amp;Load Model</title>
    <link href="/2022/07/22/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-6-Save-Load-Model/"/>
    <url>/2022/07/22/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-6-Save-Load-Model/</url>
    
    <content type="html"><![CDATA[<h1 id="1-保存和加载模型"><a href="#1-保存和加载模型" class="headerlink" title="1. 保存和加载模型"></a>1. 保存和加载模型</h1><p>在本节中，我们将了解如何通过保存、加载和运行模型预测来保持模型状态。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision.models <span class="hljs-keyword">as</span> models<br></code></pre></td></tr></table></figure><h1 id="2-保存和加载模型权重"><a href="#2-保存和加载模型权重" class="headerlink" title="2. 保存和加载模型权重"></a>2. 保存和加载模型权重</h1><p>PyTorch 模型将学习到的参数存储在称为 <strong>state_dict</strong> 的内部状态字典中。这些可以通过 <strong>torch.save</strong> 方法持久化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.vgg16(pretrained=<span class="hljs-literal">True</span>)<br>torch.save(model.state_dict(), <span class="hljs-string">&#x27;model_weights.pth&#x27;</span>)<br></code></pre></td></tr></table></figure><p>要加载模型权重，需要先创建相同模型的实例，然后使用 <strong>load_state_dict()</strong> 方法加载参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.vgg16() <br><span class="hljs-comment"># 我们不指定 pretrained=True，即不加载默认权重</span><br>model.load_state_dict(torch.load(<span class="hljs-string">&#x27;model_weights.pth&#x27;</span>))<br>model.<span class="hljs-built_in">eval</span>()<br></code></pre></td></tr></table></figure><p><strong>Note</strong>：请确保在推理前调用 <strong>model.eval()</strong> 方法，将 dropout 和 batch normalization 层设置为评估模式。如果不这样做，将产生不一致的推理结果。</p><h1 id="3-保存和加载带形状的模型"><a href="#3-保存和加载带形状的模型" class="headerlink" title="3. 保存和加载带形状的模型"></a>3. 保存和加载带形状的模型</h1><p>加载模型权重时，我们需要先实例化模型类，因为该类定义了网络的结构。我们可能希望将此类的结构与模型一起保存，在这种情况下，我们可以将模型（而不是 **model.state_dict()**）传递给保存函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(model, <span class="hljs-string">&#x27;model.pth&#x27;</span>)<br></code></pre></td></tr></table></figure><p>然后我们可以像这样加载模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model = torch.load(<span class="hljs-string">&#x27;model.pth&#x27;</span>)<br></code></pre></td></tr></table></figure><p>Note：这种方法在序列化模型时使用 Python <a href="https://docs.python.org/3/library/pickle.html">pickle</a> 模块，因此它依赖于在加载模型时可用的实际类定义。</p><h1 id="4-相关教程"><a href="#4-相关教程" class="headerlink" title="4. 相关教程"></a>4. 相关教程</h1><p><a href="https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html">在 PyTorch 中保存和加载常规检查点</a>。</p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch官方文档学习笔记_5_Optimization</title>
    <link href="/2022/07/22/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-5-Optimization/"/>
    <url>/2022/07/22/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-5-Optimization/</url>
    
    <content type="html"><![CDATA[<h1 id="0-优化模型参数"><a href="#0-优化模型参数" class="headerlink" title="0. 优化模型参数"></a>0. 优化模型参数</h1><p>在之前的内容已经介绍了模型的建立和自动微分的有关内容了，现在可以利用数据通过优化参数来训练模型了。训练模型是一个迭代过程；在每次迭代（称为<strong>epoch</strong>）中，模型对输出进行猜测，计算猜测和实际标签的误差，收集误差关于其参数的导数，并使用梯度下降优化这些参数。</p><h1 id="1-先决条件代码"><a href="#1-先决条件代码" class="headerlink" title="1. 先决条件代码"></a>1. 先决条件代码</h1><p>我们从前面的 <a href="https://blog.csdn.net/m0_60882889/article/details/124058818?spm=1001.2014.3001.5501">Datasets &amp; DataLoaders</a> 和 <a href="https://blog.csdn.net/m0_60882889/article/details/124100983?spm=1001.2014.3001.5501">Build Model</a> 部分加载代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor, Lambda<br><br>training_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">True</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br><br>test_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">False</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br><br>train_dataloader = DataLoader(training_data, batch_size=<span class="hljs-number">64</span>)<br>test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">NeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(NeuralNetwork, self).__init__()<br>        self.flatten = nn.Flatten()<br>        self.linear_relu_stack = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">28</span>*<span class="hljs-number">28</span>, <span class="hljs-number">512</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">10</span>),<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.flatten(x)<br>        logits = self.linear_relu_stack(x)<br>        <span class="hljs-keyword">return</span> logits<br><br>model = NeuralNetwork()<br></code></pre></td></tr></table></figure><h1 id="2-超参数"><a href="#2-超参数" class="headerlink" title="2. 超参数"></a>2. 超参数</h1><p>您可以通过调整超参数来控制模型优化过程。不同的超参数值会影响模型训练和收敛速度。（<a href="https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html">这里</a>可以阅读更多有关超参数调整的信息）</p><p>在这里我们为训练定义了以下超参数：</p><ul><li><strong>epoch的数值</strong>：迭代数据集的次数。</li><li>**批量大小(Batch Size)**：每次参数更新前通过网络传播的数据样本数。</li><li><strong>学习率</strong>：在每个批次&#x2F;epoch更新模型参数的比例。较小的值会产生较慢的学习速度，而较大的值可能会导致训练期间出现不可预测的行为。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">learning_rate = <span class="hljs-number">1e-3</span><br>batch_size = <span class="hljs-number">64</span><br>epochs = <span class="hljs-number">5</span><br></code></pre></td></tr></table></figure><h1 id="3-优化循环"><a href="#3-优化循环" class="headerlink" title="3. 优化循环"></a>3. 优化循环</h1><p>我们设置超参数后，就可以使用优化循环来训练和优化我们的模型。优化循环的每次迭代称为一个<strong>epoch</strong>。</p><p>每一个<strong>epoch</strong>包括两个主要部分：</p><ul><li><strong>训练循环</strong>：迭代训练数据集并尝试收敛到最优参数。</li><li><strong>验证&#x2F;测试循环</strong>：迭代测试数据集以检查模型性能是否正在提高。</li></ul><p>​    这里我们简要熟悉一下训练循环中使用的一些概念。完整实现请查看<strong>第五部分</strong>。</p><h1 id="4-损失函数"><a href="#4-损失函数" class="headerlink" title="4. 损失函数"></a>4. 损失函数</h1><p><strong>损失函数</strong>是衡量得到的结果与目标值的相异程度，我们的训练过程就是要最小化的损失函数。为了计算损失，我们使用给定数据样本的输入进行预测，并将其与真实数据标签值进行比较。</p><p>常见的损失函数包括用于回归任务的 <a href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss">nn.MSELoss</a>（均方误差）和用于分类的 <a href="https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss">nn.NLLLoss</a>（负对数似然）。 <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss">nn.CrossEntropyLoss</a> 结合了 <strong>nn.LogSoftmax</strong> 和 <strong>nn.NLLLoss</strong>。</p><p>我们将模型的输出 logits 传递给 <strong>nn.CrossEntropyLoss</strong>，它将对 logits 进行归一化并计算预测误差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Initialize the loss function</span><br>loss_fn = nn.CrossEntropyLoss()<br></code></pre></td></tr></table></figure><h1 id="5-优化器"><a href="#5-优化器" class="headerlink" title="5. 优化器"></a>5. 优化器</h1><p>优化是在每个训练步骤中调整模型参数以减少模型误差的过程。<strong>优化算法</strong>定义了如何执行这个过程（在本例中，我们使用随机梯度下降）。</p><p>所有优化逻辑都封装在 <strong><code>optimizer</code></strong> 对象中。在这里，我们使用 SGD 优化器；此外，PyTorch 中有许多<a href="https://pytorch.org/docs/stable/optim.html">不同的优化器</a>可用，例如 ADAM 和 RMSProp，它们可以更好地用于不同类型的模型和数据。</p><p>我们通过登记需要训练的模型参数来初始化优化器，并传入学习率超参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)<br></code></pre></td></tr></table></figure><p>在训练循环中，优化分三个步骤进行：</p><ul><li>调用 <strong>optimizer.zero_grad()</strong> 来重置模型参数的梯度。默认情况下渐变加起来；为了防止重复计算，我们在每次迭代时明确地将它们归零。</li><li>通过调用 <strong>loss.backward()</strong> 对预测损失进行反向传播。PyTorch将与损失有关的每个参数的梯度储存起来。</li><li>一旦我们有了梯度，我们调用 <strong>optimizer.step()</strong> 来通过反向传播中收集的梯度来调整参数。</li></ul><h1 id="6-完整实现"><a href="#6-完整实现" class="headerlink" title="6. 完整实现"></a>6. 完整实现</h1><p>我们定义了循环优化代码的 <strong>train_loop</strong>，以及针对测试数据评估模型性能的 <strong>test_loop</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_loop</span>(<span class="hljs-params">dataloader, model, loss_fn, optimizer</span>):<br>    size = <span class="hljs-built_in">len</span>(dataloader.dataset)<br>    <span class="hljs-keyword">for</span> batch, (X, y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dataloader):<br>        <span class="hljs-comment"># Compute prediction and loss</span><br>        pred = model(X)<br>        loss = loss_fn(pred, y)<br><br>        <span class="hljs-comment"># Backpropagation</span><br>        optimizer.zero_grad()<br>        loss.backward()<br>        optimizer.step()<br><br>        <span class="hljs-keyword">if</span> batch % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            loss, current = loss.item(), batch * <span class="hljs-built_in">len</span>(X)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">&#123;loss:&gt;7f&#125;</span>  [<span class="hljs-subst">&#123;current:&gt;5d&#125;</span>/<span class="hljs-subst">&#123;size:&gt;5d&#125;</span>]&quot;</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_loop</span>(<span class="hljs-params">dataloader, model, loss_fn</span>):<br>    size = <span class="hljs-built_in">len</span>(dataloader.dataset)<br>    num_batches = <span class="hljs-built_in">len</span>(dataloader)<br>    test_loss, correct = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> dataloader:<br>            pred = model(X)<br>            test_loss += loss_fn(pred, y).item()<br>            correct += (pred.argmax(<span class="hljs-number">1</span>) == y).<span class="hljs-built_in">type</span>(torch.<span class="hljs-built_in">float</span>).<span class="hljs-built_in">sum</span>().item()<br><br>    test_loss /= num_batches<br>    correct /= size<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Test Error: \n </span><br><span class="hljs-string">            Accuracy: <span class="hljs-subst">&#123;(<span class="hljs-number">100</span>*correct):&gt;<span class="hljs-number">0.1</span>f&#125;</span>%, Avg loss: <span class="hljs-subst">&#123;test_loss:&gt;8f&#125;</span> \n&quot;</span>)<br></code></pre></td></tr></table></figure><p>我们初始化损失函数和优化器，并将其传递给 <strong>train_loop</strong> 和 <strong>test_loop</strong>。随意增加 epoch 的数量来跟踪模型的改进性能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">loss_fn = nn.CrossEntropyLoss()<br>optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)<br><br>epochs = <span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">&#123;t+<span class="hljs-number">1</span>&#125;</span>\n-------------------------------&quot;</span>)<br>    train_loop(train_dataloader, model, loss_fn, optimizer)<br>    test_loop(test_dataloader, model, loss_fn)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Done!&quot;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch官方文档学习笔记_4_Autograd</title>
    <link href="/2022/07/22/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-Autograd/"/>
    <url>/2022/07/22/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-Autograd/</url>
    
    <content type="html"><![CDATA[<h1 id="1-使用-torch-autograd-进行自动微分"><a href="#1-使用-torch-autograd-进行自动微分" class="headerlink" title="1. 使用 torch.autograd 进行自动微分"></a>1. 使用 <strong>torch.autograd</strong> 进行自动微分</h1><p>在训练神经网络时，最常用的算法是反向传播。在该算法中，参数（模型权重）根据损失函数相对于给定参数的梯度进行调整。</p><p>为了计算这些梯度，PyTorch 有一个名为 <strong>torch.autograd</strong> 的内置微分引擎。它支持任何计算图的梯度自动计算。</p><p>考虑最简单的一层神经网络，输入 x、参数 w 和 b，以及一些损失函数。它可以通过以下方式在 PyTorch 中定义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br>x = torch.ones(<span class="hljs-number">5</span>)  <span class="hljs-comment"># input tensor</span><br>y = torch.zeros(<span class="hljs-number">3</span>)  <span class="hljs-comment"># expected output</span><br>w = torch.randn(<span class="hljs-number">5</span>, <span class="hljs-number">3</span>, requires_grad=<span class="hljs-literal">True</span>)<br>b = torch.randn(<span class="hljs-number">3</span>, requires_grad=<span class="hljs-literal">True</span>)<br>z = torch.matmul(x, w)+b<br>loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)<br></code></pre></td></tr></table></figure><h1 id="2-张量、函数和计算图"><a href="#2-张量、函数和计算图" class="headerlink" title="2. 张量、函数和计算图"></a>2. 张量、函数和计算图</h1><p>上述代码定义了以下计算图：</p><center>![computer_graph](/post_img/Pytorch官方文档学习笔记_4_Autograd/computer_graph.png)</center><p>在这个网络中，$w$ 和 $b$ 是我们需要优化的参数。因此，我们需要能够计算损失函数相对于这些变量的梯度。为此，我们设置了这些张量的 <strong>requires_grad</strong> 属性。</p><p>Note: 可以在创建张量时设置 <strong>requires_grad</strong> 的值，或者稍后使用 <strong>x.requires_grad_(True)</strong> 方法设置。</p><p>我们应用于张量以构建计算图的函数实际上是Function类的对象。这个对象知道如何计算正向的函数，以及如何在反向传播步骤中计算它的导数。对反向传播函数的引用存储在张量的 <strong>grad_fn</strong> 属性中。您可以在<a href="https://pytorch.org/docs/stable/autograd.html#function">这里</a>中找到更多关于 Function 的信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Gradient function for z = <span class="hljs-subst">&#123;z.grad_fn&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Gradient function for loss = <span class="hljs-subst">&#123;loss.grad_fn&#125;</span>&quot;</span>)<br><br>out: <br>&gt;&gt; Gradient function <span class="hljs-keyword">for</span> z = &lt;AddBackward0 <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7f885ddab210</span>&gt;<br>&gt;&gt; Gradient function <span class="hljs-keyword">for</span> loss = &lt;BinaryCrossEntropyWithLogitsBackward0 <span class="hljs-built_in">object</span> at <span class="hljs-number">0x7f885d8a1b90</span>&gt;<br></code></pre></td></tr></table></figure><h1 id="3-计算梯度"><a href="#3-计算梯度" class="headerlink" title="3. 计算梯度"></a>3. 计算梯度</h1><p>为了优化神经网络中参数的权重，我们需要计算我们的损失函数对参数的导数。为了计算这些导数，我们调用 **loss.backward()**，然后从 <strong>w.grad</strong> 和 <strong>b.grad</strong> 中取值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">loss.backward()<br><span class="hljs-built_in">print</span>(w.grad)<br><span class="hljs-built_in">print</span>(b.grad)<br><br>out: <br>&gt;&gt; tensor([[<span class="hljs-number">0.1702</span>, <span class="hljs-number">0.0790</span>, <span class="hljs-number">0.0602</span>],<br>           [<span class="hljs-number">0.1702</span>, <span class="hljs-number">0.0790</span>, <span class="hljs-number">0.0602</span>],<br>           [<span class="hljs-number">0.1702</span>, <span class="hljs-number">0.0790</span>, <span class="hljs-number">0.0602</span>],<br>           [<span class="hljs-number">0.1702</span>, <span class="hljs-number">0.0790</span>, <span class="hljs-number">0.0602</span>],<br>           [<span class="hljs-number">0.1702</span>, <span class="hljs-number">0.0790</span>, <span class="hljs-number">0.0602</span>]])<br>&gt;&gt; tensor([<span class="hljs-number">0.1702</span>, <span class="hljs-number">0.0790</span>, <span class="hljs-number">0.0602</span>])<br></code></pre></td></tr></table></figure><p><strong>Note:</strong></p><ul><li>我们只能获取计算图叶节点的 <strong>grad</strong> 属性，其中 <strong>requires_grad</strong> 属性设置为 <strong>True</strong>。对于我们图中的所有其他节点，梯度将不可用。</li><li>出于性能原因，我们只能在给定的图上使用一次反向梯度计算。如果我们需要对同一个图进行多次反向调用，我们需要将 <strong>retain_graph&#x3D;True</strong> 传递给反向调用。</li></ul><h1 id="4-禁用梯度跟踪"><a href="#4-禁用梯度跟踪" class="headerlink" title="4. 禁用梯度跟踪"></a>4. 禁用梯度跟踪</h1><p>默认情况下，所有 <strong>requires_grad&#x3D;True</strong> 的张量都在跟踪它们的计算历史并支持梯度计算。但是，在某些情况下我们不需要这样做，例如，当我们训练了模型并且只想将其应用于某些输入数据时，即我们只想通过网络进行前向计算。我们可以通过用 <strong>torch.no_grad()</strong> 块包围我们的计算代码来停止跟踪计算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">z = torch.matmul(x, w)+b<br><span class="hljs-built_in">print</span>(z.requires_grad)<br><br><span class="hljs-keyword">with</span> torch.no_grad():<br>    z = torch.matmul(x, w)+b<br><span class="hljs-built_in">print</span>(z.requires_grad)<br><br>out: <br>&gt;&gt; <span class="hljs-literal">True</span><br>&gt;&gt; <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><p>实现相同结果的另一种方法是在张量上使用 detach() 方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">z = torch.matmul(x, w)+b<br>z_det = z.detach()<br><span class="hljs-built_in">print</span>(z_det.requires_grad)<br><br>out: <br>&gt;&gt; <span class="hljs-literal">False</span><br></code></pre></td></tr></table></figure><p>您可能希望禁用梯度跟踪的原因如下：</p><ul><li>将神经网络中的某些参数标记为冻结参数。这是微调预训练网络的一个非常常见的场景。</li><li>当您只进行前向传播时加快计算速度，因为在不跟踪梯度的张量上进行计算会更有效。</li></ul><h1 id="5-关于计算图的更多内容"><a href="#5-关于计算图的更多内容" class="headerlink" title="5. 关于计算图的更多内容"></a>5. 关于计算图的更多内容</h1><p>从概念上讲，<strong>autograd</strong>在一个由Function对象组成的有向无环图（DAG）中保存了数据（张量）和所有执行的操作（以及产生的新张量）的记录。在DAG中，叶子是输入张量，根部是输出张量。通过追踪这个图从根到叶，你可以使用链式规则自动计算梯度。</p><p>在前向传播中，<strong>autograd</strong> 同时做两件事：</p><ul><li>运行请求操作以计算结果张量</li><li>在 DAG 中保持操作的梯度函数</li></ul><p>当在DAG根部调用.backward()时，反向传播就会启动：</p><ul><li>从每个 <strong>.grad_fn</strong> 计算梯度</li><li>将它们累积在相应张量的 <strong>.grad</strong> 属性中</li><li>使用链式法则，一直传播到叶张量</li></ul><p><strong>Note</strong>: <strong>DAG 在 PyTorch 中是动态的</strong> 需要注意的是，图形是从头开始重新创建的；在每次调用 <strong>.backward()</strong> 后，autograd开始填充一个新的图形。这正是允许你在模型中使用控制流语句的原因；如果需要，你可以在每次迭代时改变形状、大小和操作。</p><h1 id="选读：张量梯度和雅可比积"><a href="#选读：张量梯度和雅可比积" class="headerlink" title="选读：张量梯度和雅可比积"></a>选读：张量梯度和雅可比积</h1><p>在许多情况下，我们有一个标量损失函数，我们需要计算相对于某些参数的梯度。然而，有些情况下，输出函数是一个任意的张量。在这种情况下，PyTorch允许你计算所谓的<strong>雅可比乘积</strong>，而不是实际的梯度。</p><p>对于一个向量函数$\vec y&#x3D;f(\vec x)$，其中$\vec x&#x3D;&lt;x_1,…,x_2&gt;$，$\vec y&#x3D;&lt;y_1,…,y_2&gt;$，$\vec y$ 的梯度是关于 $\vec x$ 的雅可比矩阵给出的：<br>$$<br>J&#x3D;\left(\begin{array}{ccc}<br>\frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}} \<br>\vdots &amp; \ddots &amp; \vdots \<br>\frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}<br>\end{array}\right)<br>$$<br>对于给定的输入向量$\vec v&#x3D;(v_1,…,v_m)$，PyTorch允许您计算雅可比乘积 $v^T\cdot J$ 而不是计算雅可比矩阵本身。这是通过使用 $v$ 作为参数调用 <strong>backward</strong> 来实现的。 $v$​  的大小应该与我们想要计算乘积的原始张量的大小相同：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">inp = torch.eye(<span class="hljs-number">5</span>, requires_grad=<span class="hljs-literal">True</span>)<br>out = (inp+<span class="hljs-number">1</span>).<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>)<br>out.backward(torch.ones_like(inp), retain_graph=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;First call\n<span class="hljs-subst">&#123;inp.grad&#125;</span>&quot;</span>)<br>out.backward(torch.ones_like(inp), retain_graph=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\nSecond call\n<span class="hljs-subst">&#123;inp.grad&#125;</span>&quot;</span>)<br>inp.grad.zero_()<br>out.backward(torch.ones_like(inp), retain_graph=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\nCall after zeroing gradients\n<span class="hljs-subst">&#123;inp.grad&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>Out: </p><blockquote><p>First call<br> tensor([[4., 2., 2., 2., 2.],<br>       [2., 4., 2., 2., 2.],<br>       [2., 2., 4., 2., 2.],<br>       [2., 2., 2., 4., 2.],<br>       [2., 2., 2., 2., 4.]])</p><p>Second call<br> tensor([[8., 4., 4., 4., 4.],<br>       [4., 8., 4., 4., 4.],<br>       [4., 4., 8., 4., 4.],<br>       [4., 4., 4., 8., 4.],<br>       [4., 4., 4., 4., 8.]])</p><p>Call after zeroing gradients<br> tensor([[4., 2., 2., 2., 2.],<br>       [2., 4., 2., 2., 2.],<br>       [2., 2., 4., 2., 2.],<br>       [2., 2., 2., 4., 2.],<br>       [2., 2., 2., 2., 4.]])</p></blockquote><p>请注意，当我们第二次以相同的参数调用 <strong>backward</strong> 时，梯度的值是不同的。这是因为在进行反向传播时，PyTorch会累积梯度，也就是说，计算出的梯度值会加到计算图的所有叶子节点的梯度属性中。如果你想计算正确的梯度，你需要在之前将梯度属性清零。在现实的训练中，优化器可以帮助我们做到这一点。</p><p><strong>Note</strong>: 之前我们在调用 <strong>backward()</strong> 函数的时候是没有参数的。这基本上等同于调用 <strong>backward(torch.tensor(1.0))</strong> ，这是在标量值函数的情况下计算梯度的有效方法，比如神经网络训练中的损失。</p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch官方文档学习笔记_3_Build Model</title>
    <link href="/2022/07/21/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-Build-Model/"/>
    <url>/2022/07/21/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-Build-Model/</url>
    
    <content type="html"><![CDATA[<h1 id="1-建立神经网络"><a href="#1-建立神经网络" class="headerlink" title="1. 建立神经网络"></a>1. 建立神经网络</h1><p><a href="https://pytorch.org/docs/stable/nn.html">torch.nn</a> 提供了构建神经网络所需的所有构建块。PyTorch中的每个模块都是 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">nn.Module</a> 的子类。</p><p>在接下来的部分中，我们将构建一个神经网络来对 FashionMNIST 数据集中的图像进行分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets, transforms<br></code></pre></td></tr></table></figure><h1 id="2-获取训练设备"><a href="#2-获取训练设备" class="headerlink" title="2. 获取训练设备"></a>2. 获取训练设备</h1><p>我们通常希望能够在GPU等硬件加速器（如果可用）上训练模型。首先检查一下 torch.cuda 是否可用，否则就继续使用CPU。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span><br></code></pre></td></tr></table></figure><h1 id="3-定义类"><a href="#3-定义类" class="headerlink" title="3. 定义类"></a>3. 定义类</h1><p>我们通过子类化 <strong>nn.Module</strong> 来定义我们的神经网络，并在 <strong><strong>init</strong></strong> 中初始化神经网络层。每个 <strong>nn.Module</strong> 子类都在 <strong>forward</strong> 方法中实现对输入数据的操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">NeuralNetwork</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(NeuralNetwork, self).__init__()<br>        self.flatten = nn.Flatten()<br>        self.linear_relu_stack = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">28</span>*<span class="hljs-number">28</span>, <span class="hljs-number">512</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">512</span>),<br>            nn.ReLU(),<br>            nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">10</span>),<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.flatten(x)<br>        logits = self.linear_relu_stack(x)<br>        <span class="hljs-keyword">return</span> logits<br></code></pre></td></tr></table></figure><p>我们创建一个 <strong>NeuralNetwork</strong> 实例，并将其移动到 <strong>device</strong> 上，并打印其结构。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">model = NeuralNetwork().to(device)<br><span class="hljs-built_in">print</span>(model)<br><br>out: <br>&gt;&gt; NeuralNetwork(<br>     (flatten): Flatten(start_dim=<span class="hljs-number">1</span>, end_dim=-<span class="hljs-number">1</span>)<br>     (linear_relu_stack): Sequential(<br>       (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">784</span>, out_features=<span class="hljs-number">512</span>, bias=<span class="hljs-literal">True</span>)<br>       (<span class="hljs-number">1</span>): ReLU()<br>       (<span class="hljs-number">2</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">512</span>, bias=<span class="hljs-literal">True</span>)<br>       (<span class="hljs-number">3</span>): ReLU()<br>       (<span class="hljs-number">4</span>): Linear(in_features=<span class="hljs-number">512</span>, out_features=<span class="hljs-number">10</span>, bias=<span class="hljs-literal">True</span>)<br>     )<br>   )<br></code></pre></td></tr></table></figure><p>要使用模型，我们将输入数据传递给它。这将执行模型的转发，以及一些后台操作。</p><p>注意：请不要直接调用 <strong>model.forward()</strong> ！</p><p>在输入上调用模型会返回一个 10 维张量，其中包含每个类的原始预测值。我们通过一个 <strong>nn.Softmax</strong> 模块的实例来获得预测概率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, device=device)<br>logits = model(X)<br>pred_probab = nn.Softmax(dim=<span class="hljs-number">1</span>)(logits)<br>y_pred = pred_probab.argmax(<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Predicted class: <span class="hljs-subst">&#123;y_pred&#125;</span>&quot;</span>)<br><br>out: <br>&gt;&gt; Predicted <span class="hljs-keyword">class</span>: tensor([<span class="hljs-number">9</span>], device=<span class="hljs-string">&#x27;cuda:0&#x27;</span>)<br></code></pre></td></tr></table></figure><h1 id="4-模型层"><a href="#4-模型层" class="headerlink" title="4. 模型层"></a>4. 模型层</h1><p>让我们分解 FashionMNIST 模型中的层。为了方便说明，我们将抽取 3 张大小为 28x28 的图像的小批量样本，看看当我们通过网络传递它时会发生什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">input_image = torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)<br></code></pre></td></tr></table></figure><h2 id="4-1-nn-Flatten"><a href="#4-1-nn-Flatten" class="headerlink" title="4.1 nn.Flatten"></a>4.1 nn.Flatten</h2><p>我们初始化 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html">nn.Flatten</a> 层以将每个 2D 28x28 图像转换为 784 个像素值的连续数组（保持小批量维度（$dim&#x3D;0$））。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">flatten = nn.Flatten()<br>flat_image = flatten(input_image)<br><span class="hljs-built_in">print</span>(flat_image.size())<br><br>out: <br>&gt;&gt; torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">784</span>])<br></code></pre></td></tr></table></figure><h2 id="4-2-nn-Linear"><a href="#4-2-nn-Linear" class="headerlink" title="4.2 nn.Linear"></a>4.2 nn.Linear</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html">linear</a> 层是一个使用其存储的权重和偏置对输入进行线性转换的模块，即 $y&#x3D;wx+b$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">layer1 = nn.Linear(in_features=<span class="hljs-number">784</span>, out_features=<span class="hljs-number">20</span>)<br>hidden1 = layer1(flat_image)<br><span class="hljs-built_in">print</span>(hidden1.size())<br><br>&gt;&gt; torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">20</span>])<br></code></pre></td></tr></table></figure><h2 id="4-3-nn-ReLU"><a href="#4-3-nn-ReLU" class="headerlink" title="4.3 nn.ReLU"></a>4.3 nn.ReLU</h2><p>依靠非线性激活函数可以在模型的输入和输出之间创建复杂映射，在线性变换后应用以引入非线性，可以帮助神经网络学习各种复杂的现象，更多常见的激活函数及其介绍可见<a href="https://blog.csdn.net/m0_60882889/article/details/120502609?spm=1001.2014.3001.5501">这里</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">hidden1 = nn.ReLU()(hidden1)<br></code></pre></td></tr></table></figure><h2 id="4-4-nn-Sequential"><a href="#4-4-nn-Sequential" class="headerlink" title="4.4 nn.Sequential"></a>4.4 nn.Sequential</h2><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html">nn.Sequential</a> 是一个有序的模块容器。数据按照定义的顺序通过所有模块，您可以使用顺序容器来组合一个如同 <strong>seq_modules</strong> 的快速网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">seq_modules = nn.Sequential(<br>    flatten,<br>    layer1,<br>    nn.ReLU(),<br>    nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">10</span>)<br>)<br>input_image = torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)<br>logits = seq_modules(input_image)<br></code></pre></td></tr></table></figure><h2 id="4-5-nn-Softmax"><a href="#4-5-nn-Softmax" class="headerlink" title="4.5 nn.Softmax"></a>4.5 nn.Softmax</h2><p>上述的神经网络的最后一层的线性层返回了取值在 [-infty, infty] 的未处理的数值 —— <em>logits</em>。将 <em>logits</em> 传给 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html">nn.Softmax</a> 模块后取值被缩放到 [0, 1] 内，以表示模型对每个类别的预测概率。<strong>dim</strong> 参数指示维度的数值总和必须为 1 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">softmax = nn.Softmax(dim=<span class="hljs-number">1</span>)<br>pred_probab = softmax(logits)<br></code></pre></td></tr></table></figure><h1 id="5-模型参数"><a href="#5-模型参数" class="headerlink" title="5. 模型参数"></a>5. 模型参数</h1><p>神经网络中的许多层都是参数化的，即具有在训练期间优化的相关权重和偏差。子类化 nn.Module 会自动跟踪模型对象中定义的所有字段，并使用模型的 parameters() 或 named_parameters() 方法使所有参数都可以访问。</p><p>在此示例中，我们遍历每个参数，并打印其大小和其值的预览。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Model structure: <span class="hljs-subst">&#123;model&#125;</span>\n\n&quot;</span>)<br><br><span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Layer: <span class="hljs-subst">&#123;name&#125;</span> | Size: <span class="hljs-subst">&#123;param.size()&#125;</span> | Values : <span class="hljs-subst">&#123;param[:<span class="hljs-number">2</span>]&#125;</span> \n&quot;</span>)<br></code></pre></td></tr></table></figure><p>Out： </p><blockquote><p>Model structure: NeuralNetwork(<br>  (flatten): Flatten(start_dim&#x3D;1, end_dim&#x3D;-1)<br>  (linear_relu_stack): Sequential(<br>   (0): Linear(in_features&#x3D;784, out_features&#x3D;512, bias&#x3D;True)<br>   (1): ReLU()<br>   (2): Linear(in_features&#x3D;512, out_features&#x3D;512, bias&#x3D;True)<br>   (3): ReLU()<br>   (4): Linear(in_features&#x3D;512, out_features&#x3D;10, bias&#x3D;True)<br>  )<br> )</p><p> Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0033, -0.0081, -0.0354,  …, -0.0335,  0.0070,  0.0030],<br>     [ 0.0106, -0.0064,  0.0300,  …,  0.0071, -0.0062,  0.0169]],<br>     device&#x3D;’cuda:0’, grad_fn&#x3D;<SliceBackward0>)</p><p>Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0193, -0.0153], device&#x3D;’cuda:0’, grad_fn&#x3D;<SliceBackward0>)</p><p>Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0408,  0.0078,  0.0300,  …,  0.0058, -0.0142, -0.0226],<br>     [ 0.0319, -0.0063, -0.0093,  …, -0.0096,  0.0352,  0.0178]],<br>     device&#x3D;’cuda:0’, grad_fn&#x3D;<SliceBackward0>)</p><p>Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0219, 0.0020], device&#x3D;’cuda:0’, grad_fn&#x3D;<SliceBackward0>)</p><p>Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0076,  0.0076,  0.0433,  …,  0.0178,  0.0230,  0.0227],<br>     [-0.0396, -0.0042,  0.0342,  …, -0.0364, -0.0184, -0.0329]],<br>     device&#x3D;’cuda:0’, grad_fn&#x3D;<SliceBackward0>)</p><p>Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0380, -0.0044], device&#x3D;’cuda:0’, grad_fn&#x3D;<SliceBackward0>)</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch官方文档学习笔记_2_Datasets&amp;DataLoaders&amp;Transforms</title>
    <link href="/2022/07/21/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-Datasets-DataLoaders-Transforms/"/>
    <url>/2022/07/21/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-Datasets-DataLoaders-Transforms/</url>
    
    <content type="html"><![CDATA[<p>PyTorch提供了两个数据原语允许您使用预加载的数据和您自己的数据：<strong>torch.utils.data.DataLoader</strong> 和 <strong>torch.utils.data.Dataset</strong> 。</p><p>PyTorch域库提供了许多预加载的数据集（例如 FashionMNIST），这些数据集是 <strong>torch.utils.data.Dataset</strong> 的子类，并对于特定数据实现了特定的功能。它们可用于对您的模型进行原型设计和基准测试。可以在此处找到它们：<a href="https://pytorch.org/vision/stable/datasets.html">图像数据集</a>、<a href="https://pytorch.org/text/stable/datasets.html">文本数据集</a>和<a href="https://pytorch.org/audio/stable/datasets.html">音频数据集</a>。</p><h1 id="1-加载数据集"><a href="#1-加载数据集" class="headerlink" title="1. 加载数据集"></a>1. 加载数据集</h1><p>下面以如何从 TorchVision 加载 Fashion-MNIST 数据集为例。Fashion-MNIST 由 60,000 个训练示例和 10,000 个测试示例组成。每个示例都包含 28×28 灰度图像和来自 10 个类别之一的相关标签。</p><p>我们使用以下参数加载 FashionMNIST 数据集：</p><ul><li><strong>root</strong> 是存储训练&#x2F;测试数据的路径</li><li><strong>train</strong> 指定训练或测试数据集（True表示训练数据集，False表示测试数据集）</li><li><strong>download&#x3D;True</strong> 指如果根目录下不可用，则从 Internet 下载数据</li><li><strong>transform</strong> 和 <strong>target_transform</strong> 指定了特征和标签的转换。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><br>training_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">True</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br><br>test_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">False</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br></code></pre></td></tr></table></figure><h1 id="2-迭代和可视化数据集"><a href="#2-迭代和可视化数据集" class="headerlink" title="2. 迭代和可视化数据集"></a>2. 迭代和可视化数据集</h1><p>可以如同 list 一样手动索引 <strong>Datasets : training_data[index]。</strong>我们使用 <strong>matplotlib</strong> 来可视化我们训练数据中的一些样本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">labels_map = &#123;<br>    <span class="hljs-number">0</span>: <span class="hljs-string">&quot;T-Shirt&quot;</span>,<br>    <span class="hljs-number">1</span>: <span class="hljs-string">&quot;Trouser&quot;</span>,<br>    <span class="hljs-number">2</span>: <span class="hljs-string">&quot;Pullover&quot;</span>,<br>    <span class="hljs-number">3</span>: <span class="hljs-string">&quot;Dress&quot;</span>,<br>    <span class="hljs-number">4</span>: <span class="hljs-string">&quot;Coat&quot;</span>,<br>    <span class="hljs-number">5</span>: <span class="hljs-string">&quot;Sandal&quot;</span>,<br>    <span class="hljs-number">6</span>: <span class="hljs-string">&quot;Shirt&quot;</span>,<br>    <span class="hljs-number">7</span>: <span class="hljs-string">&quot;Sneaker&quot;</span>,<br>    <span class="hljs-number">8</span>: <span class="hljs-string">&quot;Bag&quot;</span>,<br>    <span class="hljs-number">9</span>: <span class="hljs-string">&quot;Ankle Boot&quot;</span>,<br>&#125;<br>figure = plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br>cols, rows = <span class="hljs-number">3</span>, <span class="hljs-number">3</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, cols * rows + <span class="hljs-number">1</span>):<br>    sample_idx = torch.randint(<span class="hljs-built_in">len</span>(training_data), size=(<span class="hljs-number">1</span>,)).item()<br>    img, label = training_data[sample_idx]<br>    figure.add_subplot(rows, cols, i)<br>    plt.title(labels_map[label])<br>    plt.axis(<span class="hljs-string">&quot;off&quot;</span>)<br>    plt.imshow(img.squeeze(), cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><center><img src="/post_img/Pytorch官方文档学习笔记_2_Datasets&DataLoaders&Transforms/picture1.png" alt="img" style="zoom:50%;" /></center><h1 id="3-创建自定义数据集"><a href="#3-创建自定义数据集" class="headerlink" title="3. 创建自定义数据集"></a>3. 创建自定义数据集</h1><p>自定义数据集类必须实现三个函数：*<strong>init</strong><em>、</em><strong>len</strong>* 和 *<strong>getitem</strong>*。看一下如下这个实现，FashionMNIST 图像存储在目录 <strong>img_dir</strong> 中，它们的标签存储在 CSV 文件 <strong>annotations_file</strong> 中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> torchvision.io <span class="hljs-keyword">import</span> read_image<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomImageDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, annotations_file, img_dir, </span><br><span class="hljs-params">                    transform=<span class="hljs-literal">None</span>, target_transform=<span class="hljs-literal">None</span></span>):<br>        self.img_labels = pd.read_csv(annotations_file)<br>        self.img_dir = img_dir<br>        self.transform = transform<br>        self.target_transform = target_transform<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.img_labels)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="hljs-number">0</span>])<br>        image = read_image(img_path)<br>        label = self.img_labels.iloc[idx, <span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> self.transform:<br>            image = self.transform(image)<br>        <span class="hljs-keyword">if</span> self.target_transform:<br>            label = self.target_transform(label)<br>        <span class="hljs-keyword">return</span> image, label<br></code></pre></td></tr></table></figure><p>接下来我们将分解介绍每个部分发生了什么。</p><h2 id="3-1-init"><a href="#3-1-init" class="headerlink" title="3.1 init"></a>3.1 <strong>init</strong></h2><p><strong>init</strong> 函数在实例化 Dataset 对象时运行一次。我们初始化了包含图像、注释文件和两种转换的目录。</p><h2 id="3-2-len"><a href="#3-2-len" class="headerlink" title="3.2 len"></a>3.2 <strong>len</strong></h2><p><strong>len</strong> 函数返回了我们数据集中的样本数。</p><h2 id="3-3-getitem"><a href="#3-3-getitem" class="headerlink" title="3.3 getitem"></a>3.3 <strong>getitem</strong></h2><p><strong>getitem</strong> 函数从给定索引 idx 的数据集中加载并返回一个样本。根据索引，它识别图像在磁盘上的位置，使用 <strong>read_image</strong> 将其转换为张量，从 <strong>self.img_labels</strong> 中的 csv 数据中检索相应的标签，调用它们的转换函数（如果适用），并返回张量图像和元组中的相应标签。</p><h1 id="4-使用-DataLoaders-为训练准备数据"><a href="#4-使用-DataLoaders-为训练准备数据" class="headerlink" title="4. 使用 DataLoaders 为训练准备数据"></a>4. 使用 DataLoaders 为训练准备数据</h1><p><strong>Dataset</strong> 每次检索一个样本的数据集特征和标签。在训练模型时，我们通常希望以“小批量的形式传递样本，在每个 epoch 重新洗牌以减少模型过度拟合，并使用 Python 的 <strong>multiprocessing</strong> 来加速数据检索。</p><p><strong>DataLoader</strong> 是一个迭代器，它通过一个简单的 API 为我们抽象了这种复杂性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br>train_dataloader = DataLoader(training_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br>test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h1 id="5-遍历-DataLoader"><a href="#5-遍历-DataLoader" class="headerlink" title="5. 遍历 DataLoader"></a>5. 遍历 DataLoader</h1><p>下面的每次迭代都会返回一批 train_features 和 train_labels（分别包含 batch_size&#x3D;64 个特征和标签）。因为我们指定了 shuffle&#x3D;True，所以在遍历所有批次之后，数据会被打乱（为了更细粒度地控制数据加载顺序，请查看 <a href="https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler">Samplers</a>）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Display image and label.</span><br>train_features, train_labels = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(train_dataloader))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Feature batch shape: <span class="hljs-subst">&#123;train_features.size()&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Labels batch shape: <span class="hljs-subst">&#123;train_labels.size()&#125;</span>&quot;</span>)<br>img = train_features[<span class="hljs-number">0</span>].squeeze()<br>label = train_labels[<span class="hljs-number">0</span>]<br>plt.imshow(img, cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br>plt.show()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Label: <span class="hljs-subst">&#123;label&#125;</span>&quot;</span>)<br><br><br>out: <br>&gt;&gt; Feature batch shape: torch.Size([<span class="hljs-number">64</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br>&gt;&gt; Labels batch shape: torch.Size([<span class="hljs-number">64</span>])<br>&gt;&gt; Label: <span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><center><img src="/post_img/Pytorch官方文档学习笔记_2_Datasets&DataLoaders&Transforms/picture2.png" style="zoom:50%;" /></center><h1 id="6-Transforms"><a href="#6-Transforms" class="headerlink" title="6. Transforms"></a>6. Transforms</h1><p>数据通常不是以训练机器学习算法所需的最终处理的形式出现。我们使用 <strong>transforms</strong> 来对数据进行一些操作并使其适合训练。</p><p>所有 TorchVision 数据集都有两个参数 —— 用于修改特征的 <strong>transform</strong> 和用于修改标签的 <strong>target_transform</strong> —— 接受包含转换逻辑的可调用对象。<a href="https://pytorch.org/vision/stable/transforms.html">torchvision.transforms</a>模块提供了几个开箱即用的常用转换。</p><p>FashionMNIST 特征是 PIL 图像格式，标签是整数。为了训练我们需要将特征作为归一化张量，并将标签作为 one-hot 编码张量。为了进行这些转换，我们使用 <strong>ToTensor</strong> 和 <strong>Lambda</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor, Lambda<br><br>ds = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">True</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor(),<br>    target_transform=Lambda(<span class="hljs-keyword">lambda</span> y: <br>            torch.zeros(<span class="hljs-number">10</span>,dtype=torch.<span class="hljs-built_in">float</span>).scatter_(<span class="hljs-number">0</span>,torch.tensor(y),value=<span class="hljs-number">1</span>)<br>    )<br>)<br></code></pre></td></tr></table></figure><h2 id="6-1-ToTensor"><a href="#6-1-ToTensor" class="headerlink" title="6.1 ToTensor()"></a>6.1 ToTensor()</h2><p>ToTensor 将 PIL 图像或 NumPy ndarray 转换为 FloatTensor，并在 [0., 1.] 范围内缩放图像的像素强度值。</p><h2 id="6-2-Lambda-Transforms"><a href="#6-2-Lambda-Transforms" class="headerlink" title="6.2 Lambda Transforms"></a>6.2 Lambda Transforms</h2><p>Lambda 转换可使用任何用户定义的 lambda 函数。这里，我们定义了一个函数来将整数转换为 one-hot 编码张量。它首先创建一个大小为 10（我们数据集中的标签数量）的零张量，并调用 **scatter_**，它在标签 y 给定的索引上分配 value&#x3D;1。</p>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch官方文档学习笔记_1_Tensor</title>
    <link href="/2022/07/21/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-Tensor/"/>
    <url>/2022/07/21/Pytorch%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-Tensor/</url>
    
    <content type="html"><![CDATA[<h1 id="1-初始化Tnesor"><a href="#1-初始化Tnesor" class="headerlink" title="1. 初始化Tnesor"></a>1. 初始化Tnesor</h1><h2 id="1-1-直接从数据"><a href="#1-1-直接从数据" class="headerlink" title="1.1 直接从数据"></a>1.1 直接从数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">data = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br>x_data = torch.tensor(data)<br></code></pre></td></tr></table></figure><h2 id="1-2-来自Numpy数组"><a href="#1-2-来自Numpy数组" class="headerlink" title="1.2 来自Numpy数组"></a>1.2 来自Numpy数组</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">np_array = np.array(data)<br>x_np = torch.from_numpy(np_array)<br></code></pre></td></tr></table></figure><h2 id="1-3-来自另一个Tensor"><a href="#1-3-来自另一个Tensor" class="headerlink" title="1.3 来自另一个Tensor"></a>1.3 来自另一个Tensor</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># retains the properties of x_data</span><br>x_ones = torch.ones_like(x_data) <br><br><span class="hljs-comment"># overrides the datatype of x_data</span><br>x_rand = torch.rand_like(x_data, dtype=torch.<span class="hljs-built_in">float</span>) <br></code></pre></td></tr></table></figure><h2 id="1-4-使用随机值或固定值"><a href="#1-4-使用随机值或固定值" class="headerlink" title="1.4 使用随机值或固定值"></a>1.4 使用随机值或固定值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">shape = (<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,)<br>rand_tensor = torch.rand(shape)<br>ones_tensor = torch.ones(shape)<br>zeros_tensor = torch.zeros(shape)<br></code></pre></td></tr></table></figure><h1 id="2-Tensor的属性"><a href="#2-Tensor的属性" class="headerlink" title="2. Tensor的属性"></a>2. Tensor的属性</h1><p>Tensor属性描述了它们的形状、数据类型和存储它们的设备。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor = torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br><br><span class="hljs-built_in">print</span>(tensor.shape)<br><span class="hljs-built_in">print</span>(tensor.dtype)<br><span class="hljs-built_in">print</span>(tensor.device)<br><br>out：<br>&gt;&gt; torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>])<br>&gt;&gt; torch.float32<br>&gt;&gt; cpu<br></code></pre></td></tr></table></figure><h1 id="3-Tensor的运算"><a href="#3-Tensor的运算" class="headerlink" title="3. Tensor的运算"></a>3. Tensor的运算</h1><p>更全面的Tensor运算介绍可点击<a href="https://pytorch.org/docs/stable/torch.html">这里</a>。</p><p>所有Tensor运算均可以在GPU上运行，并且通常有比CPU更高的速度。默认情况下，Tensor是在 CPU 上创建的。我们需要使用 <strong>.to</strong> 方法（在检查 GPU 可用性之后）将Tensor显式移动到 GPU上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    tensor = tensor.to(<span class="hljs-string">&quot;cuda&quot;</span>)<br></code></pre></td></tr></table></figure><h2 id="3-1-类似-numpy-的索引和切片"><a href="#3-1-类似-numpy-的索引和切片" class="headerlink" title="3.1 类似 numpy 的索引和切片"></a>3.1 类似 numpy 的索引和切片</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">tensor = torch.ones(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;First row: <span class="hljs-subst">&#123;tensor[<span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;First column: <span class="hljs-subst">&#123;tensor[:, <span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Last column: <span class="hljs-subst">&#123;tensor[..., -<span class="hljs-number">1</span>]&#125;</span>&quot;</span>)<br>tensor[:,<span class="hljs-number">1</span>] = <span class="hljs-number">0</span><br><span class="hljs-built_in">print</span>(tensor)<br><br>out：<br>&gt;&gt; First row: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>&gt;&gt; First column: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>&gt;&gt; Last column: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>&gt;&gt; tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>           [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>           [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>           [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br></code></pre></td></tr></table></figure><h2 id="3-2-链接Tensor"><a href="#3-2-链接Tensor" class="headerlink" title="3.2 链接Tensor"></a>3.2 链接Tensor</h2><p>可以使用 <strong>torch.cat</strong> 沿给定维度连接一系列Tensor。此外，还有一个与 <strong>torch.cat</strong> 略有不同的，可以在新的维度链接Tensor的操作可参阅<a href="https://blog.csdn.net/m0_60882889/article/details/120899946?spm=1001.2014.3001.5501">这里</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">t1 = torch.cat([tensor, tensor, tensor], dim=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(t1)<br><br>out：<br>&gt;&gt; tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>           [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>           [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>           [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br></code></pre></td></tr></table></figure><h2 id="3-3-算术运算"><a href="#3-3-算术运算" class="headerlink" title="3.3 算术运算"></a>3.3 算术运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 计算两个矩阵相乘。 y1, y2, y3具有相同的值。</span><br>y1 = tensor @ tensor.T <span class="hljs-comment"># tensor.T是计算Tensor的转置</span><br>y2 = tensor.matmul(tensor.T)<br><br>y3 = torch.rand_like(tensor) <span class="hljs-comment"># tensor.shape是两矩阵相乘后的shape</span><br>torch.matmul(tensor, tensor2.T, out=y)<br><br><br><span class="hljs-comment"># 这计算的是矩阵元素逐一相乘。 z1, z2, z3具有相同的值。</span><br>z1 = tensor * tensor<br>z2 = tensor.mul(tensor)<br><br>z3 = torch.rand_like(tensor)<br>torch.mul(tensor, tensor, out=z3)<br></code></pre></td></tr></table></figure><h2 id="3-4-转化为Python-数值"><a href="#3-4-转化为Python-数值" class="headerlink" title="3.4 转化为Python 数值"></a>3.4 转化为Python 数值</h2><p>通过 item() 可以讲单元素Tensor转化为Python数值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">agg = tensor.<span class="hljs-built_in">sum</span>() <span class="hljs-comment"># 通过将tensor的所有值聚合成一个值</span><br>agg_item = agg.item()<br><span class="hljs-built_in">print</span>(agg_item, <span class="hljs-built_in">type</span>(agg_item))<br><br>out: <br>&gt;&gt; <span class="hljs-number">12.0</span> &lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;float&#x27;</span>&gt;<br></code></pre></td></tr></table></figure><h2 id="3-5-就地操作"><a href="#3-5-就地操作" class="headerlink" title="3.5 就地操作"></a>3.5 就地操作</h2><p>将操作结果储存到操作数的操作成为就地操作。常用_后缀表示，例如：x.copy_(y)，x.t_()，会直接改变x。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(tensor, <span class="hljs-string">&quot;\n&quot;</span>)<br>tensor.add_(<span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(tensor)<br><br>out: <br>&gt;&gt;tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>          [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>          [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>          [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br><br><br>&gt;&gt;tensor([[<span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>],<br>          [<span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>],<br>          [<span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>],<br>          [<span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>]])<br></code></pre></td></tr></table></figure><p>Note: 就地操作可以节约一些内存，但是计算导数时可能出现问题，因为会立即丢失历史记录。因此不鼓励使用它们。</p><h1 id="4-与Numpy桥接"><a href="#4-与Numpy桥接" class="headerlink" title="4. 与Numpy桥接"></a>4. 与Numpy桥接</h1><p>CPU上的Tensor和Numpy数组可以共享他们的底层内存位置，并且改变其中一个另一个也会改变。</p><h2 id="4-1-Tensor到Numpy数组"><a href="#4-1-Tensor到Numpy数组" class="headerlink" title="4.1 Tensor到Numpy数组"></a>4.1 Tensor到Numpy数组</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">t = torch.ones(<span class="hljs-number">5</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;t: <span class="hljs-subst">&#123;t&#125;</span>&quot;</span>)<br>n = t.numpy()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;n: <span class="hljs-subst">&#123;n&#125;</span>&quot;</span>)<br><br>out: <br>&gt;&gt; t: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>&gt;&gt; n: [<span class="hljs-number">1.</span> <span class="hljs-number">1.</span> <span class="hljs-number">1.</span> <span class="hljs-number">1.</span> <span class="hljs-number">1.</span>]<br></code></pre></td></tr></table></figure><p>Tensor的变化会反映在Numpy数组中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">t.add_(<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;t: <span class="hljs-subst">&#123;t&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;n: <span class="hljs-subst">&#123;n&#125;</span>&quot;</span>)<br><br>out: <br>&gt;&gt; t: tensor([<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>])<br>&gt;&gt; n: [<span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span>]<br></code></pre></td></tr></table></figure><h2 id="4-2-Numpy数组到Tensor"><a href="#4-2-Numpy数组到Tensor" class="headerlink" title="4.2 Numpy数组到Tensor"></a>4.2 Numpy数组到Tensor</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">n = np.ones(<span class="hljs-number">5</span>)<br>t = torch.from_numpy(n)<br></code></pre></td></tr></table></figure><p>NumPy 数组的变化反映在Tensor中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">np.add(n, <span class="hljs-number">1</span>, out=n)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;t: <span class="hljs-subst">&#123;t&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;n: <span class="hljs-subst">&#123;n&#125;</span>&quot;</span>)<br><br>out: <br>&gt;&gt; t: tensor([<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>], dtype=torch.float64)<br>&gt;&gt; n: [<span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span>]<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>人工智能</category>
      
      <category>pytorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
